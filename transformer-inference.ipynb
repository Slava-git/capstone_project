{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8844076,"sourceType":"datasetVersion","datasetId":5323031},{"sourceId":8851655,"sourceType":"datasetVersion","datasetId":5328141},{"sourceId":8853047,"sourceType":"datasetVersion","datasetId":5328987},{"sourceId":71699,"sourceType":"modelInstanceVersion","modelInstanceId":59893},{"sourceId":71961,"sourceType":"modelInstanceVersion","modelInstanceId":60119},{"sourceId":72032,"sourceType":"modelInstanceVersion","modelInstanceId":60177},{"sourceId":72082,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":60210}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nimport ast\nimport json\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-03T22:34:24.331011Z","iopub.execute_input":"2024-07-03T22:34:24.332005Z","iopub.status.idle":"2024-07-03T22:34:24.338063Z","shell.execute_reply.started":"2024-07-03T22:34:24.331942Z","shell.execute_reply":"2024-07-03T22:34:24.336963Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import AutoTokenizer,AutoModelForSequenceClassification\nfrom transformers import LongformerTokenizer, LongformerForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:00.035777Z","iopub.execute_input":"2024-07-03T22:36:00.036495Z","iopub.status.idle":"2024-07-03T22:36:00.041637Z","shell.execute_reply.started":"2024-07-03T22:36:00.036461Z","shell.execute_reply":"2024-07-03T22:36:00.040316Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# !zip -r /kaggle/working/deberta-v3-base-tokenizer.zip /kaggle/working/deberta-v3-base-tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:01.626059Z","iopub.execute_input":"2024-07-03T22:36:01.626458Z","iopub.status.idle":"2024-07-03T22:36:01.631248Z","shell.execute_reply.started":"2024-07-03T22:36:01.626427Z","shell.execute_reply":"2024-07-03T22:36:01.629992Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\ntest_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\nsample_submission_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:01.797330Z","iopub.execute_input":"2024-07-03T22:36:01.797696Z","iopub.status.idle":"2024-07-03T22:36:03.574927Z","shell.execute_reply.started":"2024-07-03T22:36:01.797665Z","shell.execute_reply":"2024-07-03T22:36:03.574035Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model_path = '/kaggle/input/deberta-base/pytorch/deberta/1/deberta-full.pth'\n\ntokenizer_path = '/kaggle/input/deberta-tokenizer/kaggle/working/deberta-v3-base-tokenizer'","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:03.576769Z","iopub.execute_input":"2024-07-03T22:36:03.577189Z","iopub.status.idle":"2024-07-03T22:36:03.582191Z","shell.execute_reply.started":"2024-07-03T22:36:03.577151Z","shell.execute_reply":"2024-07-03T22:36:03.581014Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:07.394382Z","iopub.execute_input":"2024-07-03T22:36:07.395133Z","iopub.status.idle":"2024-07-03T22:36:07.804341Z","shell.execute_reply.started":"2024-07-03T22:36:07.395102Z","shell.execute_reply":"2024-07-03T22:36:07.803276Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class ComparisonModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\",  \n                                                                        num_labels=3, return_dict=True)\n        self.comparator = nn.Linear(6, 3)\n\n    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b):\n        # Forward pass for input A\n        outputs_a = self.model(input_ids=input_ids_a, attention_mask=attention_mask_a)\n        logits_a = outputs_a.logits  # Accessing logits from the output\n\n        # Forward pass for input B\n        outputs_b = self.model(input_ids=input_ids_b, attention_mask=attention_mask_b)\n        logits_b = outputs_b.logits  # Accessing logits from the output\n\n        # Concatenate logits\n        combined_logits = torch.cat((logits_a, logits_b), dim=1)\n\n        # Pass through the comparator\n        final_logits = self.comparator(combined_logits)\n\n        return final_logits","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:09.830603Z","iopub.execute_input":"2024-07-03T22:36:09.831008Z","iopub.status.idle":"2024-07-03T22:36:09.839125Z","shell.execute_reply.started":"2024-07-03T22:36:09.830963Z","shell.execute_reply":"2024-07-03T22:36:09.838111Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model = torch.load(model_path)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:10.202270Z","iopub.execute_input":"2024-07-03T22:36:10.202927Z","iopub.status.idle":"2024-07-03T22:36:10.791400Z","shell.execute_reply.started":"2024-07-03T22:36:10.202896Z","shell.execute_reply":"2024-07-03T22:36:10.790372Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"ComparisonModel(\n  (model): DebertaV2ForSequenceClassification(\n    (deberta): DebertaV2Model(\n      (embeddings): DebertaV2Embeddings(\n        (word_embeddings): Embedding(128100, 768, padding_idx=0)\n        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n        (dropout): StableDropout()\n      )\n      (encoder): DebertaV2Encoder(\n        (layer): ModuleList(\n          (0-11): 12 x DebertaV2Layer(\n            (attention): DebertaV2Attention(\n              (self): DisentangledSelfAttention(\n                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n                (pos_dropout): StableDropout()\n                (dropout): StableDropout()\n              )\n              (output): DebertaV2SelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n                (dropout): StableDropout()\n              )\n            )\n            (intermediate): DebertaV2Intermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): DebertaV2Output(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): StableDropout()\n            )\n          )\n        )\n        (rel_embeddings): Embedding(512, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n      )\n    )\n    (pooler): ContextPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (dropout): StableDropout()\n    )\n    (classifier): Linear(in_features=768, out_features=3, bias=True)\n    (dropout): StableDropout()\n  )\n  (comparator): Linear(in_features=6, out_features=3, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:11.444872Z","iopub.execute_input":"2024-07-03T22:36:11.445716Z","iopub.status.idle":"2024-07-03T22:36:11.457938Z","shell.execute_reply.started":"2024-07-03T22:36:11.445684Z","shell.execute_reply":"2024-07-03T22:36:11.457012Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"ComparisonModel(\n  (model): DebertaV2ForSequenceClassification(\n    (deberta): DebertaV2Model(\n      (embeddings): DebertaV2Embeddings(\n        (word_embeddings): Embedding(128100, 768, padding_idx=0)\n        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n        (dropout): StableDropout()\n      )\n      (encoder): DebertaV2Encoder(\n        (layer): ModuleList(\n          (0-11): 12 x DebertaV2Layer(\n            (attention): DebertaV2Attention(\n              (self): DisentangledSelfAttention(\n                (query_proj): Linear(in_features=768, out_features=768, bias=True)\n                (key_proj): Linear(in_features=768, out_features=768, bias=True)\n                (value_proj): Linear(in_features=768, out_features=768, bias=True)\n                (pos_dropout): StableDropout()\n                (dropout): StableDropout()\n              )\n              (output): DebertaV2SelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n                (dropout): StableDropout()\n              )\n            )\n            (intermediate): DebertaV2Intermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): DebertaV2Output(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): StableDropout()\n            )\n          )\n        )\n        (rel_embeddings): Embedding(512, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n      )\n    )\n    (pooler): ContextPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (dropout): StableDropout()\n    )\n    (classifier): Linear(in_features=768, out_features=3, bias=True)\n    (dropout): StableDropout()\n  )\n  (comparator): Linear(in_features=6, out_features=3, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"test_df['prompt'] = test_df['prompt'].apply(ast.literal_eval)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:19.249657Z","iopub.execute_input":"2024-07-03T22:36:19.250057Z","iopub.status.idle":"2024-07-03T22:36:19.261643Z","shell.execute_reply.started":"2024-07-03T22:36:19.250014Z","shell.execute_reply":"2024-07-03T22:36:19.260613Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"test_df['response_a'] = test_df['response_a'].apply(lambda x: json.loads(x))\n\ntest_df['response_b'] = test_df['response_b'].apply(lambda x: json.loads(x))","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:19.439175Z","iopub.execute_input":"2024-07-03T22:36:19.439587Z","iopub.status.idle":"2024-07-03T22:36:19.446815Z","shell.execute_reply.started":"2024-07-03T22:36:19.439552Z","shell.execute_reply":"2024-07-03T22:36:19.445449Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data(row):\n    # Replace NaN values with empty strings and concatenate prompt with each response\n    chat_a = \" [SEP] \".join([f\"prompt: {p if pd.notna(p) else ''} [RESPONSE_A] {r if pd.notna(r) else ''}\" for p, r in zip(row['prompt'], row['response_a'])])\n    chat_b = \" [SEP] \".join([f\"prompt: {p if pd.notna(p) else ''} [RESPONSE_B] {r if pd.notna(r) else ''}\" for p, r in zip(row['prompt'], row['response_b'])])\n\n    # Tokenize inputs\n    tokens_a = tokenizer(chat_a, max_length=512, truncation=True, padding=\"max_length\", return_tensors='pt')\n    tokens_b = tokenizer(chat_b, max_length=512, truncation=True, padding=\"max_length\", return_tensors='pt')\n    return tokens_a, tokens_b","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:25.952535Z","iopub.execute_input":"2024-07-03T22:36:25.953326Z","iopub.status.idle":"2024-07-03T22:36:25.960639Z","shell.execute_reply.started":"2024-07-03T22:36:25.953288Z","shell.execute_reply":"2024-07-03T22:36:25.959583Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def remove_surrogates(text_list):\n    cleaned_list = []\n    for text in text_list:\n        if text is None:\n            cleaned_list.append(\"\")\n        else:\n            try:\n                # Try to handle surrogates by encoding to 'utf-16' and decoding back to 'utf-8'\n                text = text.encode('utf-16', 'surrogatepass').decode('utf-16')\n                text = text.encode('utf-8', 'strict').decode('utf-8')\n            except UnicodeEncodeError:\n                # If error persists, remove characters that cannot be encoded in UTF-8\n                text = text.encode('utf-8', 'ignore').decode('utf-8')\n            cleaned_list.append(text)\n    return cleaned_list","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:26.804580Z","iopub.execute_input":"2024-07-03T22:36:26.804969Z","iopub.status.idle":"2024-07-03T22:36:26.811782Z","shell.execute_reply.started":"2024-07-03T22:36:26.804940Z","shell.execute_reply":"2024-07-03T22:36:26.810756Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"test_df['prompt'] = test_df['prompt'].apply(remove_surrogates)\ntest_df['response_a'] = test_df['response_a'].apply(remove_surrogates)\ntest_df['response_b'] = test_df['response_b'].apply(remove_surrogates)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:27.802375Z","iopub.execute_input":"2024-07-03T22:36:27.803165Z","iopub.status.idle":"2024-07-03T22:36:27.809322Z","shell.execute_reply.started":"2024-07-03T22:36:27.803131Z","shell.execute_reply":"2024-07-03T22:36:27.808186Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"test_df[['tokens_a', 'tokens_b']] = test_df.apply(lambda row: prepare_data(row), axis=1, result_type='expand')","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:27.980945Z","iopub.execute_input":"2024-07-03T22:36:27.981794Z","iopub.status.idle":"2024-07-03T22:36:28.006597Z","shell.execute_reply.started":"2024-07-03T22:36:27.981763Z","shell.execute_reply":"2024-07-03T22:36:28.005773Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\n\nclass TextComparisonDataset(Dataset):\n    def __init__(self, tokens_a, tokens_b, labels):\n        self.tokens_a = tokens_a\n        self.tokens_b = tokens_b\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return {\n            'input_ids_a': self.tokens_a[idx]['input_ids'].squeeze(),\n            'attention_mask_a': self.tokens_a[idx]['attention_mask'].squeeze(),\n            'input_ids_b': self.tokens_b[idx]['input_ids'].squeeze(),\n            'attention_mask_b': self.tokens_b[idx]['attention_mask'].squeeze(),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:28.895270Z","iopub.execute_input":"2024-07-03T22:36:28.895675Z","iopub.status.idle":"2024-07-03T22:36:28.903890Z","shell.execute_reply.started":"2024-07-03T22:36:28.895644Z","shell.execute_reply":"2024-07-03T22:36:28.902739Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens_a = test_df['tokens_a'].tolist()\ntokens_b = test_df['tokens_b'].tolist()\n\ntest_dataset = TextComparisonDataset(tokens_a, tokens_b, [0] * len(tokens_b))\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:30.465915Z","iopub.execute_input":"2024-07-03T22:36:30.466870Z","iopub.status.idle":"2024-07-03T22:36:30.472431Z","shell.execute_reply.started":"2024-07-03T22:36:30.466840Z","shell.execute_reply":"2024-07-03T22:36:30.471426Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"predictions = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        # Manually remove 'labels' since the dataset provides it as None\n        batch.pop('labels', 0)  # Safe to do, as we know it's dummy data\n\n        input_ids_a = batch['input_ids_a'].to(device)\n        attention_mask_a = batch['attention_mask_a'].to(device)\n        input_ids_b = batch['input_ids_b'].to(device)\n        attention_mask_b = batch['attention_mask_b'].to(device)\n\n        outputs = model(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n        predictions.extend(probabilities.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:31.369759Z","iopub.execute_input":"2024-07-03T22:36:31.370455Z","iopub.status.idle":"2024-07-03T22:36:32.125884Z","shell.execute_reply.started":"2024-07-03T22:36:31.370417Z","shell.execute_reply":"2024-07-03T22:36:32.125032Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"predictions_df = pd.DataFrame(predictions, columns=['winner_model_a', 'winner_model_b', 'winner_model_tie'])\n\n# Add the 'id' column from the test_df\npredictions_df['id'] = test_df['id'].values\n\n# Reorder columns to match the expected format\npredictions_df = predictions_df[['id', 'winner_model_a', 'winner_model_b', 'winner_model_tie']]","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:33.458447Z","iopub.execute_input":"2024-07-03T22:36:33.459816Z","iopub.status.idle":"2024-07-03T22:36:33.474798Z","shell.execute_reply.started":"2024-07-03T22:36:33.459771Z","shell.execute_reply":"2024-07-03T22:36:33.473353Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"predictions_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T22:36:33.573190Z","iopub.execute_input":"2024-07-03T22:36:33.573544Z","iopub.status.idle":"2024-07-03T22:36:33.581335Z","shell.execute_reply.started":"2024-07-03T22:36:33.573514Z","shell.execute_reply":"2024-07-03T22:36:33.580526Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# I've tried longformer and the memory isn't enough to process","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Try to use two gpu's to have the bigger context window","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}